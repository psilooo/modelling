{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d144427",
   "metadata": {},
   "source": [
    "# Binance BTC Futures Forecasting\n",
    "\n",
    "This notebook demonstrates how to load Binance futures BTC data from Google Drive, create features, and train a forecasting model using a PatchTST-style transformer with quantile regression. The notebook is designed to be run in Google Colab. Update the `data_dir` path to point to your dataset folder in Google Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0189ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your data directory on Google Drive\n",
    "# Update this path to where you've stored the Binance data files\n",
    "# For example: '/content/drive/MyDrive/binance_data'\n",
    "data_dir = Path('/content/drive/MyDrive/binance_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping dataset keys to subfolders located in `data_dir`\n",
    "dataset_folders = {\n",
    "    'aggtrades': Path('aggTrades'),\n",
    "    'bookdepth': Path('bookDepth'),\n",
    "    'indexpriceklines': Path('indexPriceKlines'),\n",
    "    'klines': Path('klines'),\n",
    "    'markpriceklines': Path('markPriceKlines'),\n",
    "    'metrics': Path('metrics'),\n",
    "    'premiumindexklines': Path('premiumIndexKlines'),\n",
    "    'trades': Path('trades'),\n",
    "}\n",
    "\n",
    "# Column names for each dataset\n",
    "cols = {\n",
    "    'aggtrades': ['agg_trade_id','price','quantity','first_trade_id','last_trade_id','transact_time','is_buyer_maker'],\n",
    "    'bookdepth': ['timestamp','percentage','depth','notional'],\n",
    "    'indexpriceklines': ['open_time','open','high','low','close','volume','close_time','quote_volume','count','taker_buy_volume','taker_buy_quote_volume','ignore'],\n",
    "    'klines': ['open_time','open','high','low','close','volume','close_time','quote_volume','count','taker_buy_volume','taker_buy_quote_volume','ignore'],\n",
    "    'markpriceklines': ['open_time','open','high','low','close','volume','close_time','quote_volume','count','taker_buy_volume','taker_buy_quote_volume','ignore'],\n",
    "    'metrics': ['create_time','symbol','sum_open_interest','sum_open_interest_value','count_toptrader_long_short_ratio','sum_toptrader_long_short_ratio','count_long_short_ratio','sum_taker_long_short_vol_ratio'],\n",
    "    'premiumindexklines': ['open_time','open','high','low','close','volume','close_time','quote_volume','count','taker_buy_volume','taker_buy_quote_volume','ignore'],\n",
    "    'trades': ['id','price','qty','quote_qty','time','is_buyer_maker'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(key):\n",
    "    \"\"\"Load and concatenate all Parquet files for the given dataset key.\"\"\"\n",
    "    folder_path = data_dir / dataset_folders[key]\n",
    "    parquet_files = sorted(folder_path.glob('*.parquet'))\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f'No Parquet files found in {folder_path}')\n",
    "    df = pd.concat((pd.read_parquet(p) for p in parquet_files), ignore_index=True)\n",
    "\n",
    "    # Enforce correct column names\n",
    "    if list(df.columns) != cols[key]:\n",
    "        df.columns = cols[key]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each dataset and display the first few rows\n",
    "datasets = {}\n",
    "for key in dataset_folders:\n",
    "    print(f'Loading {key}...')\n",
    "    datasets[key] = load_dataset(key)\n",
    "    display(datasets[key].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant timestamp columns to datetime\n",
    "# Kline-like data: convert open_time and close_time (milliseconds)\n",
    "for key in ['indexpriceklines','klines','markpriceklines','premiumindexklines']:\n",
    "    df = datasets[key]\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')\n",
    "    datasets[key] = df\n",
    "\n",
    "# Metrics: convert create_time to datetime\n",
    "datasets['metrics']['create_time'] = pd.to_datetime(datasets['metrics']['create_time'], unit='ms')\n",
    "\n",
    "# Book depth: convert timestamp\n",
    "datasets['bookdepth']['timestamp'] = pd.to_datetime(datasets['bookdepth']['timestamp'], unit='ms')\n",
    "\n",
    "# Agg trades: convert transact_time\n",
    "datasets['aggtrades']['transact_time'] = pd.to_datetime(datasets['aggtrades']['transact_time'], unit='ms')\n",
    "\n",
    "# Trades: convert time\n",
    "datasets['trades']['time'] = pd.to_datetime(datasets['trades']['time'], unit='ms')\n",
    "\n",
    "print('Timestamps converted.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Resample klines to 15-minute bars\n",
    "# If your klines are already 1-minute bars, this aggregates them to 15-minute intervals\n",
    "price_df = datasets['klines'][['open_time','open','high','low','close','volume','quote_volume']].copy()\n",
    "price_df = price_df.set_index('open_time').sort_index()\n",
    "\n",
    "price_15m = price_df.resample('15T').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum',\n",
    "    'quote_volume': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "price_15m.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: compute log returns and rolling volatility\n",
    "price_15m['return'] = np.log(price_15m['close'] / price_15m['close'].shift(1))\n",
    "# Rolling volatility over 3 hours (12 bars)\n",
    "price_15m['volatility'] = price_15m['return'].rolling(window=12).std() * np.sqrt(12)\n",
    "price_15m = price_15m.dropna()\n",
    "\n",
    "price_15m.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model training\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CONTEXT_LENGTH = 100  # past 100 15-min bars (~25 hours)\n",
    "HORIZON = 100         # forecast next 100 15-min bars\n",
    "\n",
    "# Select features for modelling (e.g., return and volatility)\n",
    "features = price_15m[['return','volatility']].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_norm = scaler.fit_transform(features)\n",
    "\n",
    "# Convert to tensor\n",
    "features_tensor = torch.tensor(features_norm, dtype=torch.float32)\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, context_length, horizon):\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "        self.horizon = horizon\n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.context_length + self.horizon) + 1\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.context_length]\n",
    "        y = self.data[idx + self.context_length : idx + self.context_length + self.horizon, 0]  # use return as target\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TimeSeriesDataset(features_tensor.numpy(), CONTEXT_LENGTH, HORIZON)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f'Number of training samples: {len(dataset)}')\n",
    "# Inspect shapes\n",
    "x0, y0 = dataset[0]\n",
    "print('Context shape:', x0.shape)\n",
    "print('Target shape:', y0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PatchTST-like transformer for multi-horizon quantile forecasting\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchTST(nn.Module):\n",
    "    def __init__(self, input_dim, context_length, horizon, patch_len=8, stride=4, d_model=256, n_heads=8, n_layers=4, quantiles=None):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.horizon = horizon\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.n_patches = (context_length - patch_len) // stride + 1\n",
    "        self.quantiles = quantiles or [0.05, 0.1, 0.5, 0.9, 0.95]\n",
    "        \n",
    "        # Patch embedding and transformer encoder\n",
    "        self.patch_embed = nn.Linear(patch_len * input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Output head to produce (horizon * num_quantiles) outputs\n",
    "        self.head = nn.Linear(d_model, horizon * len(self.quantiles))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        patches = []\n",
    "        for i in range(0, self.context_length - self.patch_len + 1, self.stride):\n",
    "            patch = x[:, i:i+self.patch_len, :].reshape(batch_size, -1)\n",
    "            patches.append(patch)\n",
    "        patches = torch.stack(patches, dim=1)\n",
    "        z = self.patch_embed(patches)\n",
    "        z = self.transformer(z)\n",
    "        z = z.mean(dim=1)  # global average pooling over patches\n",
    "        out = self.head(z)\n",
    "        return out.view(batch_size, self.horizon, len(self.quantiles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop skeleton\n",
    "import torch.optim as optim\n",
    "\n",
    "model = PatchTST(input_dim=features_tensor.shape[1], context_length=CONTEXT_LENGTH, horizon=HORIZON)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "quantiles = torch.tensor(model.quantiles)\n",
    "\n",
    "# Quantile loss function\n",
    "def quantile_loss(pred, target, quantiles):\n",
    "    # pred: (batch, horizon, num_quantiles)\n",
    "    # target: (batch, horizon)\n",
    "    target = target.unsqueeze(-1)\n",
    "    diff = target - pred\n",
    "    loss = torch.max(quantiles * diff, (quantiles - 1) * diff)\n",
    "    return loss.mean()\n",
    "\n",
    "# Train for a few epochs (adjust as needed)\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch = x_batch.float()\n",
    "        y_batch = y_batch.float()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_batch)\n",
    "        loss = quantile_loss(preds, y_batch, quantiles)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18259e27",
   "metadata": {},
   "source": [
    "## Conformal Calibration\n",
    "\n",
    "After training your quantile forecasting model, you should set aside a recent portion of data as a calibration set. Use the model's predictions on the calibration set to adjust your prediction intervals via conformal quantile regression (CQR) or other conformal methods to ensure the empirical coverage matches your desired confidence level (e.g., 80% or 90%). This step will convert the raw quantile outputs into well-calibrated prediction intervals.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}