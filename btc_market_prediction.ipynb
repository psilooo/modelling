{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a375b01",
   "metadata": {},
   "source": [
    "\n",
    "# BTCUSDT Market Prediction Model\n",
    "\n",
    "This notebook demonstrates how to build a predictive model for the\n",
    "Bitcoin/USDT pair (BTCUSDT) using a rich set of market data hosted\n",
    "within a Google Drive folder.  The available datasets include trade\n",
    "tapes, order book snapshots at various depth percentages, multiple\n",
    "variants of kline (candlestick) series, and metrics describing open\n",
    "interest and funding rates.  Our goal is to produce a model that\n",
    "analyses the last 100 15‑minute candles and predicts the direction and\n",
    "magnitude of the next 100 15‑minute candles.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "\n",
    "1. **Data loading** – reads Parquet files from the drive folder\n",
    "   specified by `base_dir` and concatenates multiple months of data.\n",
    "2. **Resampling & preprocessing** – converts 1‑minute klines to\n",
    "   15‑minute bars, and computes order book top‑level imbalances,\n",
    "   depth ratios at several percentage bands, and an open interest RSI.\n",
    "3. **Feature engineering** – builds a feature matrix using the most\n",
    "   recent 100 bars of returns, rolling statistics, volume proxies,\n",
    "   order book features and premium/basis values.\n",
    "4. **Label construction** – defines binary (up/down) and continuous\n",
    "   (log‑return) targets for the next 100 bars.\n",
    "5. **Model training** – fits gradient boosted models using a\n",
    "   time‑series split for cross‑validation and prints performance\n",
    "   metrics.\n",
    "6. **Live signal** – provides a function to compute a buy/sell/hold\n",
    "   recommendation based on the latest data.\n",
    "\n",
    "You can run each cell sequentially after ensuring that the data\n",
    "directory exists on your runtime environment.  Adjust `base_dir` if\n",
    "your data live in a different location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display plots inline\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_parquet_files(base_path, subdir, pattern, parse_dt_cols=None, index_col=None):\n",
    "    \"\"\"Load multiple Parquet files from a subdirectory into a single DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_path : str\n",
    "        Root directory containing all data categories (e.g. '/content/drive/MyDrive/binance_data').\n",
    "    subdir : str\n",
    "        Name of the subdirectory within `base_path` (e.g. 'klines').\n",
    "    pattern : str\n",
    "        Glob pattern to match files (e.g. 'BTCUSDT_klines_BTCUSDT-klines-*.parquet').\n",
    "    parse_dt_cols : list of str, optional\n",
    "        List of columns to parse as datetime with UTC timezone.\n",
    "    index_col : str, optional\n",
    "        Column to set as the index after concatenation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Concatenated DataFrame containing data from all matching files.\n",
    "    \"\"\"\n",
    "    full_pattern = os.path.join(base_path, subdir, pattern)\n",
    "    paths = sorted(glob.glob(full_pattern))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No files matched pattern {full_pattern}\")\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_parquet(p)\n",
    "        if parse_dt_cols:\n",
    "            for col in parse_dt_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_datetime(df[col], utc=True)\n",
    "        dfs.append(df)\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    if index_col and index_col in df_all.columns:\n",
    "        df_all = df_all.set_index(index_col).sort_index()\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def resample_klines_to_15m(k):\n",
    "    \"\"\"Resample a DataFrame of 1-minute klines to 15-minute bars.\n",
    "\n",
    "    The resulting DataFrame will have OHLC fields aggregated over each 15-minute\n",
    "    interval, along with summed volumes and counts.\n",
    "    \"\"\"\n",
    "    # Ensure a DateTimeIndex\n",
    "    k = k.sort_index()\n",
    "    k15 = k.resample('15T', label='right', closed='right').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum',\n",
    "        'quote_volume': 'sum',\n",
    "        'count': 'sum',\n",
    "        'taker_buy_volume': 'sum',\n",
    "        'taker_buy_quote_volume': 'sum',\n",
    "    }).dropna(subset=['open', 'high', 'low', 'close'])\n",
    "    return k15\n",
    "\n",
    "\n",
    "def depth_ratio_frame(minute_df):\n",
    "    \"\"\"Compute order book imbalance features for a single minute snapshot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    minute_df : pandas.DataFrame\n",
    "        Rows representing different percentage buckets at the same timestamp.\n",
    "        Must include 'percentage' and 'depth' columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with top-of-book totals and depth ratios for various bands.\n",
    "    \"\"\"\n",
    "    bids = minute_df[minute_df['percentage'] < 0].sort_values('depth', ascending=False)\n",
    "    asks = minute_df[minute_df['percentage'] > 0].sort_values('depth', ascending=False)\n",
    "\n",
    "    top3_bids = bids['depth'].head(3).sum()\n",
    "    top3_asks = asks['depth'].head(3).sum()\n",
    "    tob_imbalance = (top3_bids - top3_asks) / (top3_bids + top3_asks) if (top3_bids + top3_asks) > 0 else 0.0\n",
    "\n",
    "    def band_ratio(lower, upper):\n",
    "        bids_sum = minute_df[(minute_df['percentage'] <= -lower) & (minute_df['percentage'] >= -upper)]['depth'].sum()\n",
    "        asks_sum = minute_df[(minute_df['percentage'] >= lower) & (minute_df['percentage'] <= upper)]['depth'].sum()\n",
    "        return (bids_sum - asks_sum) / (bids_sum + asks_sum) if (bids_sum + asks_sum) > 0 else 0.0\n",
    "\n",
    "    ratios = {\n",
    "        'depthr_0_2p5': band_ratio(0.0, 2.5),\n",
    "        'depthr_1_2p5': band_ratio(1.0, 2.5),\n",
    "        'depthr_2p5_5': band_ratio(2.5, 5.0),\n",
    "        'depthr_5_10': band_ratio(5.0, 10.0),\n",
    "    }\n",
    "    return {\n",
    "        'tob_top3_bids': top3_bids,\n",
    "        'tob_top3_asks': top3_asks,\n",
    "        'tob_imb': tob_imbalance,\n",
    "        **ratios,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_book_features_15m(book_df):\n",
    "    \"\"\"Aggregate order book snapshots into 15-minute features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    book_df : pandas.DataFrame\n",
    "        DataFrame with a 'timestamp' column (posix ms) and columns\n",
    "        'percentage', 'depth', and 'notional'.  Each unique timestamp\n",
    "        represents a snapshot containing multiple percentage buckets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame indexed by 15-minute bars, where each row contains\n",
    "        top-of-book volumes, imbalance, and depth ratios.\n",
    "    \"\"\"\n",
    "    # Convert timestamp to datetime index in UTC\n",
    "    if not isinstance(book_df.index, pd.DatetimeIndex):\n",
    "        book_df = book_df.copy()\n",
    "        book_df['timestamp'] = pd.to_datetime(book_df['timestamp'], unit='ms', utc=True)\n",
    "        book_df = book_df.set_index('timestamp')\n",
    "\n",
    "    # Compute features for each minute snapshot\n",
    "    per_minute = []\n",
    "    for ts, grp in book_df.groupby(book_df.index.floor('T')):\n",
    "        feats = depth_ratio_frame(grp)\n",
    "        feats['ts'] = ts\n",
    "        per_minute.append(feats)\n",
    "    minute_features = pd.DataFrame(per_minute).set_index('ts').sort_index()\n",
    "\n",
    "    # Forward-fill to handle missing minutes, then resample to 15 minutes\n",
    "    minute_features = minute_features.resample('T').last().ffill()\n",
    "    f15 = minute_features.resample('15T', label='right', closed='right').last()\n",
    "    return f15\n",
    "\n",
    "\n",
    "def rsi(series, n=14):\n",
    "    \"\"\"Compute the Relative Strength Index (RSI) for a price or open interest series.\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0.0)\n",
    "    loss = (-delta).clip(lower=0.0)\n",
    "    avg_gain = gain.ewm(alpha=1 / n, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=1 / n, adjust=False).mean()\n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "def make_oi_rsi_15m(metrics_df, window=14):\n",
    "    \"\"\"Compute a 14-period RSI of open interest on 15-minute intervals.\"\"\"\n",
    "    if not isinstance(metrics_df.index, pd.DatetimeIndex):\n",
    "        metrics_df = metrics_df.copy()\n",
    "        metrics_df['create_time'] = pd.to_datetime(metrics_df['create_time'], utc=True)\n",
    "        metrics_df = metrics_df.set_index('create_time').sort_index()\n",
    "    oi = metrics_df['sum_open_interest'].astype(float)\n",
    "    oi_rsi = rsi(oi, window)\n",
    "    oi_rsi_df = pd.DataFrame({'oi_rsi': oi_rsi})\n",
    "    return oi_rsi_df.resample('15T', label='right', closed='right').last()\n",
    "\n",
    "\n",
    "def make_basis_15m(mark_df, index_df):\n",
    "    \"\"\"Compute the basis (mark price minus index price) and its normalized value.\"\"\"\n",
    "    basis = mark_df['close'] - index_df['close']\n",
    "    nbasis = basis / index_df['close']\n",
    "    return pd.DataFrame({'basis': basis, 'nbasis': nbasis})\n",
    "\n",
    "\n",
    "def add_price_features(features_df, k15, num_lags=100):\n",
    "    \"\"\"Augment a feature DataFrame with price-based features.\n",
    "\n",
    "    This function computes log returns, rolling mean and standard deviation of returns,\n",
    "    realized volatility over various windows, a price RSI, and past return lags.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_df : pandas.DataFrame\n",
    "        The feature matrix to augment.  Its index should align with k15.\n",
    "    k15 : pandas.DataFrame\n",
    "        15-minute bar DataFrame with at least a 'close' column.\n",
    "    num_lags : int\n",
    "        Number of past return lags to include as separate features.\n",
    "    \"\"\"\n",
    "    k = k15.copy()\n",
    "    k['logp'] = np.log(k['close'])\n",
    "    k['ret1'] = k['logp'].diff()\n",
    "    # Rolling statistics on returns\n",
    "    for w in (4, 12, 24, 48, 96):  # 1h..24h on 15m bars\n",
    "        features_df[f'ret_mean_{w}'] = k['ret1'].rolling(w).mean()\n",
    "        features_df[f'ret_std_{w}'] = k['ret1'].rolling(w).std()\n",
    "        features_df[f'rv_{w}'] = (k['ret1'] ** 2).rolling(w).sum()\n",
    "    # Price RSI\n",
    "    features_df['rsi_14'] = rsi(k['close'], 14)\n",
    "    # Lags of returns\n",
    "    for lag in range(1, num_lags + 1):\n",
    "        features_df[f'ret_lag_{lag}'] = k['ret1'].shift(lag)\n",
    "    # Volume proxies\n",
    "    for w in (4, 12, 48):\n",
    "        features_df[f'vol_sum_{w}'] = k['volume'].rolling(w).sum()\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afdfa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Configuration ----\n",
    "base_dir = os.getenv('BINANCE_DATA_DIR', '/content/drive/MyDrive/binance_data')\n",
    "symbol = 'BTCUSDT'\n",
    "horizon = 100  # prediction horizon: 100 future 15‑minute bars\n",
    "num_lags = 100  # number of lagged return features (context length)\n",
    "\n",
    "# Ensure the data directory exists\n",
    "if not os.path.isdir(base_dir):\n",
    "    if base_dir.startswith('/content/'):\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "        except Exception as e:\n",
    "            print(f'Could not mount Google Drive automatically: {e}')\n",
    "    if not os.path.isdir(base_dir):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data directory '{base_dir}' not found. Set BINANCE_DATA_DIR environment variable to your data location.\"\n",
    "        )\n",
    "\n",
    "# ---- Load raw data ----\n",
    "# Klines (1m) to be resampled; pattern matches all monthly files\n",
    "klines = read_parquet_files(base_dir, 'klines', f\"{symbol}-klines-*.parquet\", parse_dt_cols=['open_time', 'close_time'], index_col='open_time')\n",
    "# Book depth snapshots\n",
    "bookdepth = read_parquet_files(base_dir, 'bookDepth', f\"{symbol}-bookDepth-*.parquet\")\n",
    "aggtrades = read_parquet_files(base_dir, 'aggTrades', f\"{symbol}-aggTrades-*.parquet\")\n",
    "trades = read_parquet_files(base_dir, 'trades', f\"{symbol}-trades-*.parquet\")\n",
    "# Metrics (open interest etc.)\n",
    "metrics = read_parquet_files(base_dir, 'metrics', f\"{symbol}-metrics-*.parquet\", parse_dt_cols=['create_time'])\n",
    "# Mark price klines\n",
    "markprice = read_parquet_files(base_dir, 'markPriceKlines', f\"{symbol}-markPriceKlines-*.parquet\", parse_dt_cols=['open_time', 'close_time'], index_col='open_time')\n",
    "# Index price klines\n",
    "indexprice = read_parquet_files(base_dir, 'indexPriceKlines', f\"{symbol}-indexPriceKlines-*.parquet\", parse_dt_cols=['open_time', 'close_time'], index_col='open_time')\n",
    "# Premium index klines\n",
    "premium = read_parquet_files(base_dir, 'premiumIndexKlines', f\"{symbol}-premiumIndexKlines-*.parquet\", parse_dt_cols=['open_time', 'close_time'], index_col='open_time')\n",
    "\n",
    "# ---- Resample to 15‑minute bars ----\n",
    "k15 = resample_klines_to_15m(klines)\n",
    "mark15 = markprice.resample('15T', label='right', closed='right').last()[['close']].rename(columns={'close': 'close'})\n",
    "index15 = indexprice.resample('15T', label='right', closed='right').last()[['close']].rename(columns={'close': 'close'})\n",
    "premium15 = premium.resample('15T', label='right', closed='right').last()[['close']].rename(columns={'close': 'premium'})\n",
    "\n",
    "# ---- Feature construction ----\n",
    "# Start with an empty DataFrame indexed on 15‑minute intervals\n",
    "X = pd.DataFrame(index=k15.index)\n",
    "# Price‑based features\n",
    "X = add_price_features(X, k15, num_lags=num_lags)\n",
    "# Order book features (depth ratios and top‑of‑book imbalance)\n",
    "book15 = make_book_features_15m(bookdepth)\n",
    "X = X.join(book15, how='left')\n",
    "# Open interest RSI\n",
    "oi15 = make_oi_rsi_15m(metrics)\n",
    "X = X.join(oi15, how='left')\n",
    "# Basis and normalized basis\n",
    "basis15 = make_basis_15m(mark15, index15)\n",
    "X = X.join(basis15, how='left')\n",
    "# Premium\n",
    "X = X.join(premium15, how='left')\n",
    "# Handle infinities and fill missing values\n",
    "X = X.replace([np.inf, -np.inf], np.nan).ffill(limit=5).bfill(limit=5)\n",
    "\n",
    "# ---- Create targets ----\n",
    "close_prices = k15['close']\n",
    "future_log_return = np.log(close_prices.shift(-horizon)) - np.log(close_prices)\n",
    "y_direction = (future_log_return > 0).astype(int)  # 1 if price goes up, else 0\n",
    "y_regression = future_log_return\n",
    "\n",
    "# Drop rows where labels or lags are unavailable\n",
    "valid = ~y_regression.isna()\n",
    "# Ensure all lags are present\n",
    "for lag in range(1, num_lags + 1):\n",
    "    valid &= ~X[f'ret_lag_{lag}'].isna()\n",
    "X = X[valid]\n",
    "y_direction = y_direction[valid]\n",
    "y_regression = y_regression[valid]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of classification samples: {y_direction.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # ---- Train/test split using a time‑series cross‑validation ----\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        X_values = X.values\n",
    "        y_dir_values = y_direction.values\n",
    "        y_reg_values = y_regression.values\n",
    "\n",
    "        cls_scores = []\n",
    "        reg_scores = []\n",
    "        # Out‑of‑sample predictions to evaluate overall performance\n",
    "        prob_oos = pd.Series(index=X.index, dtype=float)\n",
    "        ret_oos = pd.Series(index=X.index, dtype=float)\n",
    "\n",
    "        fold_num = 1\n",
    "        for train_idx, test_idx in tscv.split(X_values):\n",
    "            X_train, X_test = X_values[train_idx], X_values[test_idx]\n",
    "            y_train_cls, y_test_cls = y_dir_values[train_idx], y_dir_values[test_idx]\n",
    "            y_train_reg, y_test_reg = y_reg_values[train_idx], y_reg_values[test_idx]\n",
    "\n",
    "            # Classification model (direction)\n",
    "            cls_model = HistGradientBoostingClassifier(max_depth=6, learning_rate=0.1, max_iter=300, l2_regularization=0.0)\n",
    "            cls_model.fit(X_train, y_train_cls)\n",
    "            # Regression model (return magnitude)\n",
    "            reg_model = HistGradientBoostingRegressor(max_depth=6, learning_rate=0.05, max_iter=400, l2_regularization=0.0)\n",
    "            reg_model.fit(X_train, y_train_reg)\n",
    "\n",
    "            # Predictions\n",
    "            prob_pred = cls_model.predict_proba(X_test)[:, 1]\n",
    "            ret_pred = reg_model.predict(X_test)\n",
    "            prob_oos.iloc[test_idx] = prob_pred\n",
    "            ret_oos.iloc[test_idx] = ret_pred\n",
    "\n",
    "            # Metrics\n",
    "            auc = roc_auc_score(y_test_cls, prob_pred)\n",
    "            acc = accuracy_score(y_test_cls, (prob_pred > 0.5).astype(int))\n",
    "            f1 = f1_score(y_test_cls, (prob_pred > 0.5).astype(int))\n",
    "            rmse = mean_squared_error(y_test_reg, ret_pred, squared=False)\n",
    "            cls_scores.append((auc, acc, f1))\n",
    "            reg_scores.append(rmse)\n",
    "            print(f\"Fold {fold_num}: AUC={auc:.3f}, ACC={acc:.3f}, F1={f1:.3f}, RMSE={rmse:.6f}\")\n",
    "            fold_num += 1\n",
    "\n",
    "        # Overall performance summary\n",
    "        mean_auc = np.mean([s[0] for s in cls_scores])\n",
    "        mean_acc = np.mean([s[1] for s in cls_scores])\n",
    "        mean_f1 = np.mean([s[2] for s in cls_scores])\n",
    "        mean_rmse = np.mean(reg_scores)\n",
    "        print(f\"\n",
    "Average classification performance: AUC={mean_auc:.3f}, ACC={mean_acc:.3f}, F1={mean_f1:.3f}\")\n",
    "        print(f\"Average regression RMSE: {mean_rmse:.6f}\")\n",
    "\n",
    "        # Fit full models on entire dataset for deployment\n",
    "        final_cls_model = HistGradientBoostingClassifier(max_depth=6, learning_rate=0.1, max_iter=300, l2_regularization=0.0)\n",
    "        final_reg_model = HistGradientBoostingRegressor(max_depth=6, learning_rate=0.05, max_iter=400, l2_regularization=0.0)\n",
    "        final_cls_model.fit(X_values, y_dir_values)\n",
    "        final_reg_model.fit(X_values, y_reg_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        def live_signal(feature_matrix, k15_bars, cls_model, reg_model, horizon=horizon, buy_threshold=0.60, sell_threshold=0.40):\n",
    "            # Compute a trading signal based on the latest available features.\n",
    "            # A BUY signal is issued when the predicted probability of an upward\n",
    "            # move exceeds `buy_threshold`; a SELL signal is issued when the\n",
    "            # probability falls below `sell_threshold`.  Otherwise a HOLD is\n",
    "            # returned.  Confidence is reported on a scale from 0 to 1.\n",
    "            x = feature_matrix.iloc[-1].values.reshape(1, -1)\n",
    "            p_up = float(cls_model.predict_proba(x)[0, 1])\n",
    "            exp_ret = float(reg_model.predict(x)[0])\n",
    "            exp_pct = (math.exp(exp_ret) - 1.0) * 100.0\n",
    "\n",
    "            if p_up >= buy_threshold:\n",
    "                signal = 'BUY'\n",
    "                confidence = p_up\n",
    "            elif p_up <= sell_threshold:\n",
    "                signal = 'SELL'\n",
    "                confidence = 1 - p_up\n",
    "            else:\n",
    "                signal = 'HOLD'\n",
    "                # Confidence near mid-range: distance from 0.5 scaled into [0,1]\n",
    "                confidence = 0.5 + abs(p_up - 0.5)\n",
    "\n",
    "            latest_close = float(k15_bars['close'].iloc[-1])\n",
    "            target_price = latest_close * math.exp(exp_ret)\n",
    "            return {\n",
    "                'latest_close': latest_close,\n",
    "                'p_up': round(p_up, 4),\n",
    "                'expected_log_return': round(exp_ret, 6),\n",
    "                'expected_move_percent': round(exp_pct, 4),\n",
    "                'target_price_t_plus_h': round(target_price, 2),\n",
    "                'signal': signal,\n",
    "                'confidence': round(confidence, 4),\n",
    "            }\n",
    "\n",
    "        # Example usage on the last bar (after training):\n",
    "        example_signal = live_signal(X, k15, final_cls_model, final_reg_model)\n",
    "        print(\"\n",
    "Live signal based on the most recent data:\")\n",
    "        print(example_signal)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}